<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" href="../../favicon.ico">

    <title>james-irwin@github</title>

    <!-- Bootstrap core CSS -->
    <link href="dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Bootstrap theme -->
    <link href="dist/css/bootstrap-theme.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="theme.css" rel="stylesheet">
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-16765789-5', 'auto');
  ga('send', 'pageview');

</script>
  </head>

  <body role="document">

    <!-- Fixed navbar -->
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">james-irwin@github</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="#">Home</a></li>
            <li><a href="#contact">Contact</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container theme-showcase" role="main">

      <!-- Main jumbotron for a primary marketing message or call to action -->
      <div class="jumbotron">
        <h1>Somewhere to keep ...</h1>
        <p>things that are done because I need to learn by doing.</p>
      </div>


<div id="prescription"> </div>
      <div class="page-header">
          <h1>Top drug writers</h1>
      </div>
        <a href="http://james-irwin.github.io/prescriptions/medicinehub.html"><img src="thumbs/prescriptions.png" class="img-thumbnail" alt="Where are the popular spots for writing diazepam?"></a>
<div>
<p>
Is the surgery opposite the watersports park still the top place for dispensing
entonox? What are we mostly medicating prisoners with?
</p>
<p>
        <a href="http://james-irwin.github.io/prescriptions/medicinehub.html"><button type="button" class="btn btn-primary">Site</button></a> Look at which practice/pharmacy writes the most of a given drug.
    </p>
    <p>
        <a href="http://www.github.com/james-irwin/prescriptions/"><button type="button" class="btn btn-success">Code</button></a> JavaScript to aggregate records loaded into Elastic Search.
    </p>
    <p>
    <a href="http://www.hscic.gov.uk/searchcatalogue?q=title:%22presentation+level+data%22&area=&size=10&sort=Relevance"><button type="button" class="btn btn-info">Data</button></a> Monthly prescription data.
    </p>
</div>

<div>
<p>
More JavaScript in the client, with the google maps API, and a proper
aggregation in elastic search.

Last time I looked at the prescription data, there was much less. Then I used
PHP, MySQL and <a href="http://matplotlib.org/">matplotlib</a> wrapped in shell
scripts to make heatmaps of prescription density.

The <a href="http://www.hscic.gov.uk/searchcatalogue?q=title:%22presentation+level+data%22&area=&size=10&sort=Relevance">data</a>
arrives in <a href="http://datagov.ic.nhs.uk/presentation/2015_07_July/T201507PDPI+BNFT.CSV">monthly CSV chunks</a>.
</p>
<pre>
Q44,RTV,Y04937,0204000R0AAAHAH,Propranolol HCl_Tab 10mg,0000001,00000001.45,00000001.35,0000028,201507,
Q44,RTV,Y04937,0401020K0AAAIAI,Diazepam_Tab 5mg,0000001,00000000.80,00000000.85,0000021,201507,
</pre>
<p>
Of that, I kept the practice writing the drug, a short version of the BNF code
and the quantity.
</p>
<pre>
Y04937,0204000R0,28,
Y04937,0401020K0,21,
</pre>
<p>
This means 5mg, 10mg and 20mg tablets of the same thing are bucketed together,
I know that this misrepresents the tonnage but I'm not weighing it, just looking
at the popularity of writing the drug itself. I chose the quantity column,
ignoring the items column through gut feel with items mostly being low numbers.
Turning this into something to bulk load into elastic was done in AWK with
trivial print statements selecting field numbers, adding curly brackets and
quotes.
</p>
<pre>
{ "index" : { "_index" : "presc", "_type": "ription", "_id" : "2"}}
{"practice" : "Y04937", "bnf" : "0204000R0", "quantity" : 28}
{ "index" : { "_index" : "presc", "_type": "ription", "_id" : "4"}}
{"practice" : "Y04937", "bnf" : "0401020K0", "quantity" : 21}
</pre>
<p>
I think I could have trimmed off the index and type fields by putting them
in the URL used in the bulk load. This made a 20m line intermediate ouput in
two minutes for
the July 2015 data set. The bulk API was good for 100,000 line chunks (50,000
documents). Chopping the 20m row file took 75 seconds (a little C program that
greedily loads all of it and then fwrites it back out in blocks to numbered
files).
Loading these into elastic was complete in eight minutes (two or three seconds
per file of 50,000 entries).
</p>
<pre>
$>time seq 207 | while read chunk
> do
> time curl -XPOST localhost:9200/_bulk --data-binary @chunk-$chunk.dat
> done
real    8m0.354s
</pre>
<p>
My macbook pro took about twelve minute to prepare and ingest one month of
prescription data.
I'm not trending the data over time, I'm looking at which practices are writing
the most of a particular drug, or for a given practice, their most popular
presription item. This core data is ready and loaded at this point but it
needs more help to be interesting. I mean, that drug 0410030C0 is written the
most by practice e87637 is half a job. I'm going to put that place on a map,
along with it's top ten siblings, along with all other practices that have this
drug in its top ten written items, and I'd like to know that those drugs are
in English.
The practice address data, unlike the Food
Standards Agency data, doesn't include lat-long for placing markers. It looks
like regular (and not always cleansed) keyed data.
</p>
<pre>
201507,Y02315,DMC COMMUNITY MINOR SURGERY SERVICE,SOUTH WOODFORD HEALTH CTR,114 HIGH ROAD,SOUTH WOODFORD,LONDON,E18 2QS
</pre>
<p>
There's a BNF to English CSV file too, and it's at a higher level than that
in the prescription data (part of why I bucketed 5mg, 10mg etc. together).
</p>
<pre>
0401020G0,Bromazepam,
0401020K0,Diazepam,
0401020L0,Ketazolam,
0401020P0,Lorazepam,
0401020Q0,Medazepam,
0401020R0,Meprobamate,
0401020T0,Oxazepam,
0401020U0,Prazepam,
</pre>
<p>
The practice data was turned into a collection of JSON objects that I could
then extend by adding lat/long for plotting and ultimately the most popular
scripts for that location. Lat/log came from google's geocoding API.
</p>
<pre>
curl "http://maps.googleapis.com/maps/api/geocode/json?address=SOUTH%20WOODFORD%20HEALTH%20CTR%20114%20HIGH%20ROAD%20E182QS" -o Y02315.json
</pre>
<p>
Google limit the number of geo-inversions to 2500 per day, so I batched the 9898
practices in the data set.



Then it's the top ten drugs they write.

THen it' teh top ten places that write a given drug same query, just rotated
and it's the code in teh github associated here.

Done.

-----

Load the data into ES is AWKing the CSV file.

Turning the practice names into things to plot is awk + google geocoding.

geocoding with 10k locations had to be done in stages -- limits. Could
ultimately to 'top 10' for any given drug dynamicaly but couldn't really
do the 'all' button.

Finding the top ten prescribers of a drug and the top ten prescriptions for
a given practices is elastic aggregates.

Give timings for the steps and hte number of rows. Sows that this could
be calculated real-time -- here however we have historical data with a
reasonable number of rows for a demonstration.

</p>
</div>



<div id="wordpairs"> </div>
      <div class="page-header">
          <h1>Stylometry &amp; stylo-sympathy</h1>
      </div>
        <a href="http://james-irwin.github.io/word-pairs/"><img src="thumbs/word-pairs.png" class="img-thumbnail" alt="Write-like and style-comparison of texts."></a>
<div>
    <p>
        I wondered how real stylometry might be and, better than that, could a machine help me write sympathetically in a given style? It arose from wondering about simultaneous multiple auto-suggest feeds and <a href="https://www.washingtonpost.com/world/national-security/code-name-verax-snowden-in-exchanges-with-post-reporter-made-clear-he-knew-risks/2013/06/09/c9a25b54-d14c-11e2-9f1a-1a7cdee20287_story.html">the request to not be quoted at length</a>.
    </p>
<p>
        <a href="http://james-irwin.github.io/word-pairs/"><button type="button" class="btn btn-primary">Site</button></a> Write-like a few classics and compare them too.
    </p>
    <p>
        <a href="http://www.github.com/james-irwin/word-pairs/"><button type="button" class="btn btn-success">Code</button></a> JavaScript to generate word-distribution distillates; compare them and consume them.
    </p>
    <p>
    <a href="http://www.gutenberg.org"><button type="button" class="btn btn-info">Data</button></a> Project Gutenburg.
    </p>
</div>

<div>
    <p>
        I started with a body of standard texts:
        <a href="http://www.gutenberg.org/ebooks/10.txt.utf-8">The King James Bible</a>
        and <a href="http://www.gutenberg.org/ebooks/100.txt.utf-8">the complete works of
        Shakespeare</a>. I'd used both before as sources of word streams for benchmarking.
        Before analysing, I picked up a couple more samples that I might want to compare
        which meant <a href="http://www.gutenberg.org/ebooks/31100.txt.utf-8">all of Jane
        Austen</a> and <a href="http://www.gutenberg.org/files/30486/30486-0.txt">Bronte's
        Shirley</a> to go with Shakespeare; an
        <a href="http://www.gutenberg.org/ebooks/3434.txt.utf-8">English translation
            of the Koran</a> and an
        <a href="http://www.gutenberg.org/ebooks/7864.txt.utf-8">English translation of the Mahabharata of Krishna</a> to weigh against a partitioned Bible (old and new testaments).
        The first step was to distill the text into something to use for both
        auto-suggesting and a basis for the comparison operation. The end resut was a Markov-
        chain like structure:
    </p>
<p>
    <pre>
residue = ["word_1": ["following_word_1.1", "following_word_1.2", ... ],
           "word_2": ["following_word_2.1", "following_word_2.2", ... ], ...
          ]</pre>
</p>
    <p>
        Where the words in the structure above are frequency sorted. I dropped the occurence
        counts for precise distributions and assumed a common (power-) curve for simplicity.
    </p>
    <div class="alert alert-warning" role="alert">
    I was, as with the previous experiments, trying to keep learning JavaScript but it made no
    sense if I could do the grunt work by piping together a few binutils classics. The
    JavaScript step in this turns the text into word-pairs (or word --&gt;
    following word-pair) and thresholds the result (truncating the very long tails).
    Otherwise the punctuation was removed with <i>tr</i>; ordering and counting with
    <i>sort</i> and <i>uniq</i>; and JSON-ification with trivial <i>AWK</i>. Premature
    implementation of all the steps in JavaScript would have missed the point: To see if
    stylometry from this residue might be convincing and if I could make a sumultaneous
    multi-auto-suggest from it. The <a href="https://github.com/james-irwin/word-pairs/blob/master/tools/wpdist.sh">distillation</a>
    of the 880k words of Shakespeare takes thirty seconds. The rest were completed in a couple
    of minutes. The slowest part was manually trimming the texts of their introductory and
    explanatory envelopes.
    </div>

<p>
    First to use the output were the auto-suggesters based on Nicholas Zakas'
    <a href="http://oak.cs.ucla.edu/cs144/projects/javascript/suggest1.html">tutorial</a>.
    Using a simple probability distributon function to pick the <i>next word</i>, iterated a
    few times and while I typed, a satisfactory prompt came from the suggester.
</p>
    <div class="alert alert-danger" role="alert">
    Single-word based Markov chains read badly so I switched the distiller to produce
    following word pairs. Iterating <i>"word": ["word-pair_1", "word-pair_2", ...]</i>
    read much easier and is used on the <a href="http://www.github.com/james-irwin/word-pairs/">page</a>
    for the <i>classics</i> and <i>scripture</i> suggesters.
    </div>
<p>
To compare two Markov-chains -- I haven't read anything about how stylometry is done
properly -- I used a three dimensional metric that I could bubble-chart using
    <a href="http://canvasjs.com/">CanvasJS</a> charts for a change.
    The size of the intersection of two word lists as a percentage of the union is the y-axis
    (<i>inness</i>) and the absolute size of the intersection is the bubble size.
    This is to give an
    idea of the commonality between two texts. This doesn't eliminate inescapable common word-
    pairs of a written language but might give a sense of overlap. The third dimension (x-axis
    in the <a href="http://www.github.com/james-irwin/word-pairs/">example</a>) is the
    frequency order comparison (<i>likeness</i>) computed as a rank correlation using the
    <a href="http://simplestatistics.org/">simple statistics</a> library. Both the parallel
    suggesters and the bubble-chart with annotations are intersting to play with as a result.
</p>
</div>

<div id="enronmail"> </div>
      <div class="page-header">
          <h1>Sankey diagrams of Enron email via Elasticsearch</h1>
      </div>
        <a href="http://james-irwin.github.io/enron/"><img src="thumbs/enron.even.png" class="img-thumbnail" alt="Enron mail analytics."></a>
<div>
    <p>
        An excuse to draw Sankey diagrams and learn to do aggregations in Elasticsearch. The Enron mail corpus fit and was plenty less trendy than twitter/facebook feeds. Check out <a href="http://james-irwin.github.io/enron/vince.kaminski.batwings.html">Vince Kaminski</a> (an objector to the practices with a fat cc: pipe to himself outside of Enron) or <a href="http://james-irwin.github.io/enron/rosalee.fleming.batwings.html">Rosalee Flemming</a> (Ken Lay's assistant, a broadcast beacon to the top brass).
    </p>
<p>
        <a href="http://james-irwin.github.io/enron/"><button type="button" class="btn btn-primary">Site</button></a> Sankey diagrams (with clickable fudge).
    </p>
    <p>
        <a href="http://www.github.com/james-irwin/enronsankey/"><button type="button" class="btn btn-success">Code</button></a> JavaScript to generate Sankey diagrams from aggregates collected from Elasticsearch.
    </p>
    <p>
    <a href="https://www.cs.cmu.edu/~./enron/"><button type="button" class="btn btn-info">Data</button></a> Enron email corpus.
    </p>
</div>

<div>
    <p>
            The tarball of data needed loading into <a href="https://www.elastic.co/">Elasticsearch</a>. I used <a href="https://github.com/andris9/mailparser">mailparser</a> in processing each file to generate an Elasticsearch entry demultiplexing multiple names on the <i>To:</i> line.
    </p>
    <p>
    <pre>node mail.to.es.js .../enron/enron_mail_20110402/maildir/crandell-s/inbox/2. </pre>
    </p>
    <div class="alert alert-danger" role="alert">
    That took half a second to load the single mbox-format mail and generate forty-one documents in Elasticsearch (a sample with a long <i>To:</i> list). With half a million files the machine was left running while I floored the loft over a weekend. There's a lot of fat left on that pork chop to speed it up. In the end there are 3.1m documents in the Elasticsearch index.
    </div>
    <p>
    The simple, first Sankey aggregations I wanted was the bowtie as in the image above. This would effectively be the pair:
    </p>
<pre>SELECT 'FROM', COUNT(*) AS C WHERE 'TO'='mike.mcconnell' FROM ENRONMAIL GROUP BY 'FROM' ORDER BY C DESC;
SELECT 'TO', COUNT(*) AS C WHERE 'FROM'='mike.mcconnell' FROM ENRONMAIL GROUP BY 'TO' ORDER BY C DESC;
</pre>
<p>
    Shove that into a basic <a href="https://developers.google.com/chart/interactive/docs/gallery/sankey">Sankey</a>
    and you're away. I wanted to make the full set and have them navigable by clicking
    on the sames of the branches to recentre the bowtie. Turns out
    <a href="https://stackoverflow.com/questions/30337430/select-event-not-fired-on-google-chart-api-for-sankey">that's not yet possible</a>
    so I now have two versions -- a clickable list pair of labels with badges to
    show the counts and the Sankey image.
</p>
    <p>
    I used a modified version of the Sankey generator and iterated it a few times
    on the list of names it found to build a three-degrees from Rick Buy (or whoever)
    totaling 1826 names to then batch run the lot. Sankeyfying for those names took
    2mins 12secs. That's fast enough to do on demand.
    </p>
    <div class="alert alert-success" role="alert">
    Since these little bowtie Sankey diagrams take less than a tenth of a second each,
    I want to dial it up a step and go out one branch farther. To have a
    <i>from:from</i> feed into the <i>from</i> left hand side and the mirror
    on the right. That turns the number of aggregations up from two about the
    centre name to twenty-two.
    </div>
    <p>
    First pass at a batwing to just extend the left (from/from) helped with error
    discovery. Added lots of <tt>&lt;namestring&gt;.replace(/\'/g,"");</tt>
    for the O'Days, O'Haras, O'Keeffes and so on. I could've escaped them or
    like I have here, just dropped them.
    </p>
    <a href="http://james-irwin.github.io/enron/mike.mcconnell.batwings.html"><img src="thumbs/half.batwing.png" class="img-thumbnail" alt="Enron mail analytics."></a>
</div>
    <p>
    A matching set of batwings linked to bowtie Sankey views (click the centre name
    in the top set) generated in 2mins 59secs for twenty times as much work done in
    Elasticsearch. Still sub a tenth of a second each. Possible on demand then,
    probably while continually indexing new documents as they pass through the mail
    system.
    </p>

      <!-- END OF ELSATIC/ENRON -->

<div id="fsamap"> </div>
      <div class="page-header">
          <h1>JavaScript via the Food Standards Agency</h1>
      </div>
        <a href="http://james-irwin.github.io/fsa/"><img src="thumbs/fsa.png" class="img-thumbnail" alt="Food Standards Agency inspection ratings pinned over a Google map."></a>
<div>
    <p>
    I wanted to learn a little JavaScript and being a non-UI person
    I did <em>learnyounode</em> until I thought I had enough to start something
    in anger.</p>
    <p>
    <a href="http://james-irwin.github.io/fsa/"><button type="button" class="btn btn-primary">Site</button></a>
    FSA inspection ratings gathered by county pinned over Google's map.
    </p>
    <p>
    <a href="http://www.github.com/james-irwin/fsamap/"><button type="button" class="btn btn-success">Code</button></a>
    JavaScript to generate static HTML set over Google's mapping API.
    </p>
    <p>
    <a href="http://ratings.food.gov.uk/open-data/en-GB"><button type="button" class="btn btn-info">Data</button></a> Food Standards Agency inspection ratings data.
    </p>
</div>
<div>
<p>
I wanted all the data, and it comes in XML and JSON options. Curl it in batches via
</p>
<p>
    <pre>curl http://ratings.food.gov.uk/search/%5E/%5E/1/5000/xml</pre>
</p>
<p>
    Where <tt>%5E</tt> (^) effectively means a wildcard for both the name of the
    county and the establishment that was inspected. Replace <tt>xml</tt> with
    <tt>json</tt> for JSON formatted equivalent. <tt>1</tt> is the page number and
    <tt>5000</tt> the page size. That's the largest page size I managed to collect
    -- for a total of about a hundred pages, the result includes metadata with the
    number of pages at your selected page size. The GitHub repo has an example
    <a href="https://github.com/james-irwin/fsamap/tree/master/examples">HTML output</a>
    for just the first 5000 elements.

<div class="alert alert-danger" role="alert">
    Mushing all the FSA inspection data into one large JSON file totals
    <strong>346Mb</strong>! Throwing away most of that to just pin the location,
    the rating and the name will make a single file in the tens of megabytes range.
    Plus node didn't like <tt>fs.readfile()</tt> on a 346Mb file and I didn't have
    need/care to switch to a streamed implementation. I'm writing my first JavaScript
    program and didn't want any back-end otherwise I would use elasticsearch and mark
    the dynamically pulled inspections within the bounds of the Google map.
</div>
</p>
<p>
    As published, the data is partitioned into counties so I scraped the 406 files by
    curling the <a href = "http://ratings.food.gov.uk/open-data/en-GB" >data page</a>
    and piping it through <tt>grep http://ratings.food.gov.uk/OpenDataFiles/ | grep English</tt>
    to see something I could use <tt>cut</tt> to strip out the county names and the
    file names to re-<tt>curl</tt>. The GitHub repo has the
    <a href="https://github.com/james-irwin/fsamap/tree/master/xmldata">xml data for North Somerset</a>
    as a snapshot.
</p>
<p>
    <a href="https://developers.google.com/maps/documentation/javascript/markers">Google maps with pins are surprisingly simple</a>.
    After installing <tt>xml2js</tt> to parse the xml with:
<pre>
var xml2js = require('xml2js');
var parser = xml2js.Parser();
parser.parseString(data, function (err, result) {...
</pre>
The inspection set for this county (loaded into <tt>data</tt> via <tt>fs.readFile()</tt>) is now navigable as a JavaScript object.
<div class="alert alert-danger" role="alert">
    Parsing 406 xml files to put a few thousand pins for the county you're interested in
    plus 405 <em>jump</em>-pins at appropriate places on the map takes over
    <strong>three minutes!</strong> on my 2014 MacBook Pro. Scaling that even with the
    two cores takes about <strong>eight hours</strong> for the set.</div>

    I had overnight to let it run rather than do a smarter thing and cache the 406
    <em>jump</em>-pins. Then I realised I needed something better anyway...</div>

<div class="alert alert-danger" role="alert">
    A dozen or so maps wouldn't load, the error console bemoaning
    <strong>RangeError: Maximum call stack size exceeded</strong>. Thinking about it,
    turned out it was because I was simply emitting all the inspections and using
    <tt>map.panTo(m[0].position)</tt> -- the first marker so that the map was centred
    over a pin in the county rather than calculate the proper centre of the map first
    and then emit the cloud of markers. Heed the quality of the data!
</div>
<p>
    Wanting to avoid re-writing it with a cached set of <em>jump</em>-pins I bulk
    converted all the <tt>xml</tt> files to <tt>JSON</tt> with
    <tt>JSON.stringify()</tt> as the body to the callback of the
    <tt>xml2js::parser.parseString()</tt> function and exchanged the <tt>xml</tt>
    parser call to a <tt>JSON</tt> parser call (<tt>JSON.parse(data)</tt>). Took
    only a couple of minutes.
<div class="alert alert-success" role="alert">
    Two wins: Firstly, three minutes per county <tt>xml</tt> turned into
    <strong>seven seconds</strong> per county <tt>JSON</tt> (less than half
    an hour to do the lot now) and secondly, skipping inspections that have otherwise
    <tt>undefined</tt> lat/long data made the browser come alive and all the counties
    I tried worked.</div>
    </p>
<div class="page-header" id="contact">
    <h1>Contact</h1>
</div>
<div class="row">
    <div class="col-sm-4">
        <a href="http://uk.linkedin.com/pub/james-irwin/5/313/a03"><img src="thumbs/Logo-2C-34px-R.png" class="img-thumbnail" alt="LinkedIn profile for James Irwin."></a>
        <a href="http://www.github.com/james-irwin/"><img src="thumbs/GitHub_Logo.png" class="img-thumbnail" alt="GitHub for James Irwin."></a>

    </div><!-- /.col-sm-4 -->
</div>
      </div>


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
    <script src="dist/js/bootstrap.min.js"></script>
</script>
  </body>
</html>
